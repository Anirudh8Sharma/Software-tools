!pip install transformers
# Step 1: Model Preparation
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
# Step 2: Mount Google Drive to access your data
from google.colab import drive
drive.mount('/content/drive')
# Load the fine-tuned model
model_path = "/content/drive/MyDrive/Sherlock_model"
model = GPT2LMHeadModel.from_pretrained(model_path)

# Load the pre-trained GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Set `pad_token_id` to `eos_token_id` for open-end generation
model.config.pad_token_id = model.config.eos_token_id

# Step 2: Chatbot Interface
print("Chatbot: Hello! How can I assist you today?")
while True:
    user_input = input("You: ")
    if user_input.lower() == 'exit':
        print("Chatbot: Goodbye!")
        break

    # Encode the input text and create the attention mask
    input_ids = tokenizer.encode(user_input, return_tensors='pt')
    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)

    # Generate a response from the chatbot with an increased temperature
    with torch.no_grad():
        output = model.generate(input_ids, attention_mask=attention_mask, max_length=1000,
                                num_return_sequences=1, temperature=0.7)

    # Decode the generated output and return the response
    response = tokenizer.decode(output[0], skip_special_tokens=True)

    # Filter out repeated user input as the response
    if response.lower().strip() == user_input.lower().strip():
        print("Chatbot: I'm sorry, but I don't have an answer for that.")
    else:
        print("Chatbot:", response)
